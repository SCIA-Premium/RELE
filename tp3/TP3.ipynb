{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning\n",
    "# Cours 3 : Policy Iteration and Value Iteration\n",
    "\n",
    "Pour trouver une politique optimale, il existe deux grandes familles d'algorithmes : la programmation dynamique (résoudre le problème en le décomposant récursivement en plus petits problèmes) et les simulations de Monte-Carlo (faire des expériences pour estimer les distributions de probabilités). \n",
    "\n",
    "Dans ce TP, nous étudions deux types d'algorithme utilisant la programmation dynamique : les itérations sur les valeurs et les itérations sur la politique.\n",
    "\n",
    "\n",
    "RAPPEL : 1/4 de la note finale est liée à la mise en forme : \n",
    "\n",
    "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
    "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
    "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
    "\n",
    "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
    "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
    "\n",
    "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
    "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
    "\n",
    "```\n",
    "Nom1, Prenom1, Email1, NumEtudiant1\n",
    "Nom2, Prenom2, Email2, NumEtudiant2\n",
    "Nom3, Prenom3, Email3, NumEtudiant3\n",
    "Nom4, Prenom4, Email4, NumEtudiant4\n",
    "```\n",
    "\n",
    "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unikarah/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Estimation de la fonction de valeur d'un gridword\n",
    "\n",
    "Nous avons vu en cours que :\n",
    "\n",
    "$$v_\\pi (s) = \\mathbb{E}_\\pi \\left( G_t | s \\right) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_\\pi(s') \\right]$$\n",
    "\n",
    "Dans le cas où les dynamiques de l'environnement sont entièrement connus, $p(s'|s, a)$ peut s'exprimer sous la forme d'un tensor et l'équation précédente aboutit à un système d'équations linéaires. Le problème est donc résolvable, mais la résolution risque d'être longue si l'environnement est grand. \n",
    "\n",
    "On cherche plutôt une résolution itérative qui applique le principe de la programmation dynamique. Concrètement, on part d'une fonction de valeur arbitraire $v_0$ (par exemple nulle partout), puis on y applique à chaque étape l'équation de Bellman :\n",
    "$$v_{k+1} (s) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
    "Lorsque l'algorithme a convergé vers un point fixe $v_\\infty$, nous avons fini d'évaluer $v_\\pi$, puisque ce dernier est l'unique point fixe de la fonction de valeur.\n",
    "\n",
    "Cet algorithme est appelé l'**évaluation itérative de la politique**.\n",
    "\n",
    "On considère par la suite le \"gridworld\" suivant :\n",
    "\n",
    "![gridworld](img/grid-world.png)\n",
    "\n",
    "Les cases grisées sont terminales et la récompense est de -1 sur toutes les transitions.\n",
    "La taille du gridworld est une constante `CUBE_SIDE`.\n",
    "\n",
    "**Q1: évaluez la fonction de valeur de la politique aléatoire à l'aide d'un algorithme itératif. Arrếtez l'algorithme lorsque les valeurs n'ont pas évolué de plus de 1e-2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state values\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "\n",
      "State values after policy evaluation\n",
      "[0, 0.8, 1.7000000000000002, 2.6, 3.5, 4.175]\n",
      "[4.625, 5.300000000000001, 6.199999999999999, 7.1, 8.0, 8.675]\n",
      "[10.025, 10.7, 11.600000000000001, 12.5, 13.4, 14.075000000000003]\n",
      "[15.425, 16.1, 17.0, 17.9, 18.8, 19.475]\n",
      "[20.825, 21.5, 22.4, 23.3, 24.2, 24.875]\n",
      "[25.325000000000003, 26.0, 26.9, 27.8, 28.700000000000003, 0]\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import typing as t\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Action = t.Literal[\"L\", \"R\", \"U\" , \"D\"]\n",
    "CUBE_SIDE = 6\n",
    "\n",
    "@dataclass\n",
    "class State: \n",
    "    \"\"\"\n",
    "    It represents any cell in the world\n",
    "    \"\"\"\n",
    "    cell: int\n",
    "    value: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.bounds = {\n",
    "            'L': self.cell - self.cell % CUBE_SIDE,\n",
    "            'R': self.cell - self.cell % CUBE_SIDE + (CUBE_SIDE - 1),\n",
    "            'U': self.cell % CUBE_SIDE,\n",
    "            'D': self.cell % CUBE_SIDE + CUBE_SIDE * (CUBE_SIDE - 1),\n",
    "        }\n",
    "        self.neighbors = [self.act(a) for a in \"LRUD\"]\n",
    "        assert all(i >= 0 and i < CUBE_SIDE*CUBE_SIDE for i in self.neighbors)\n",
    "    \n",
    "    def is_termination(self):\n",
    "        return self.cell in {0, CUBE_SIDE * CUBE_SIDE - 1}\n",
    "\n",
    "    def act(self, a: Action):\n",
    "        \"\"\"\n",
    "        Get next state\n",
    "        \"\"\"\n",
    "        if a == 'L': \n",
    "            return min(self.bounds['R'], max(self.bounds['L'], self.cell - 1))\n",
    "        if a == 'R': \n",
    "            return min(self.bounds['R'], max(self.bounds['L'], self.cell + 1))\n",
    "        if a == 'U': \n",
    "            return min(self.bounds['D'], max(self.bounds['U'], self.cell - 4))\n",
    "        if a == 'D':\n",
    "            return min(self.bounds['D'], max(self.bounds['U'], self.cell + 4))\n",
    "        raise ValueError('Unexpected action')\n",
    "    \n",
    "\n",
    "def init_states():\n",
    "    return [State(i) for i in range(CUBE_SIDE * CUBE_SIDE)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Env:\n",
    "    states: t.List[State] = field(default_factory=init_states)\n",
    "\n",
    "# The probability of taking the action is 0.25 because we have 4 actions\n",
    "def policy_evaluation(env, gamma=0.9, theta=1e-2, r=-1):\n",
    "    \"\"\"\n",
    "    Policy evaluation function\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states:\n",
    "            if s.is_termination():\n",
    "                continue\n",
    "            v = s.value\n",
    "            s.value = 0\n",
    "            for a in s.neighbors:\n",
    "                s.value += 0.25 * (r + gamma * a)\n",
    "            delta = max(delta, abs(v - s.value))\n",
    "        if delta < theta:\n",
    "            break\n",
    "        \n",
    "    return env\n",
    "\n",
    "def print_states(states):\n",
    "    for i in range(CUBE_SIDE):\n",
    "        print([states[i*CUBE_SIDE + j].value for j in range(CUBE_SIDE)])\n",
    "env = Env()\n",
    "print(\"Initial state values\")\n",
    "print_states(env.states)\n",
    "\n",
    "pol_eval = policy_evaluation(env)\n",
    "\n",
    "print(\"\\nState values after policy evaluation\")\n",
    "print_states(pol_eval.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La politique gloutonne cherche uniquement à exploiter, sans aucune exploration. A chaque instant, elle choisit l'action qui permet de maximiser la fonction de valeur :\n",
    "\n",
    "$$\\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
    "\n",
    "**Q2: calculez la politique ainsi obtenue. Vérifiez qu'il s'agit de la politique optimale. Combien d'itérations ont été nécessaires pour obtenir ce résultat ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Algorithme *policy iteration*\n",
    "\n",
    "Une amélioration de l'algorithme consiste 1) à évaluer la fonction de valeur sur un petit nombre d'itérations (on testera en Q3 avec une seule itération), puis 2) à mettre à jour la politique, puis à recommencer l'étape 1). On peut arrếter l'entraînement lorsque la politique a convergé.\n",
    "\n",
    "**Q3: implémentez cet algorithme. Est-il plus rapide ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Algorithme *value iteration*\n",
    "\n",
    "Une autre variante conserve la politique aléatoire tout en long de l'entraînement, mais met à jour la fonction de valeur avec l'équation suivante :\n",
    "\n",
    "$$v_{k+1} (s) = \\max_{a} \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
    "\n",
    "Une fois que la fonction de valeur a convergé, on calcule la politique avec :\n",
    "\n",
    "$$\\pi(s) = argmax_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
    "\n",
    "\n",
    "**Q4: implémentez cet algorithme.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: Quel algorithme vous paraît le plus judicieux ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Ajoutez votre commentaire ici]*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1612589585.625011,
  "download_nb": false,
  "filename": "20_markov.rst",
  "filename_with_path": "20_markov",
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "title": "Foundations of Computational Economics #20",
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
