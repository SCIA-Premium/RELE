{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning\n",
    "# Cours 3 : Policy Iteration and Value Iteration\n",
    "\n",
    "Pour trouver une politique optimale, il existe deux grandes familles d'algorithmes : la programmation dynamique (résoudre le problème en le décomposant récursivement en plus petits problèmes) et les simulations de Monte-Carlo (faire des expériences pour estimer les distributions de probabilités). \n",
    "\n",
    "Dans ce TP, nous étudions deux types d'algorithme utilisant la programmation dynamique : les itérations sur les valeurs et les itérations sur la politique.\n",
    "\n",
    "\n",
    "RAPPEL : 1/4 de la note finale est liée à la mise en forme : \n",
    "\n",
    "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
    "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
    "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
    "\n",
    "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
    "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
    "\n",
    "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
    "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
    "\n",
    "```\n",
    "Nom1, Prenom1, Email1, NumEtudiant1\n",
    "Nom2, Prenom2, Email2, NumEtudiant2\n",
    "Nom3, Prenom3, Email3, NumEtudiant3\n",
    "Nom4, Prenom4, Email4, NumEtudiant4\n",
    "```\n",
    "\n",
    "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Estimation de la fonction de valeur d'un gridword\n",
    "\n",
    "Nous avons vu en cours que :\n",
    "\n",
    "$$v_\\pi (s) = \\mathbb{E}_\\pi \\left( G_t | s \\right) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_\\pi(s') \\right]$$\n",
    "\n",
    "Dans le cas où les dynamiques de l'environnement sont entièrement connus, $p(s'|s, a)$ peut s'exprimer sous la forme d'un tensor et l'équation précédente aboutit à un système d'équations linéaires. Le problème est donc résolvable, mais la résolution risque d'être longue si l'environnement est grand. \n",
    "\n",
    "On cherche plutôt une résolution itérative qui applique le principe de la programmation dynamique. Concrètement, on part d'une fonction de valeur arbitraire $v_0$ (par exemple nulle partout), puis on y applique à chaque étape l'équation de Bellman :\n",
    "$$v_{k+1} (s) = \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
    "Lorsque l'algorithme a convergé vers un point fixe $v_\\infty$, nous avons fini d'évaluer $v_\\pi$, puisque ce dernier est l'unique point fixe de la fonction de valeur.\n",
    "\n",
    "Cet algorithme est appelé l'**évaluation itérative de la politique**.\n",
    "\n",
    "On considère par la suite le \"gridworld\" suivant :\n",
    "\n",
    "![gridworld](img/grid-world.png)\n",
    "\n",
    "Les cases grisées sont terminales et la récompense est de -1 sur toutes les transitions.\n",
    "La taille du gridworld est une constante `CUBE_SIDE`.\n",
    "\n",
    "**Q1: évaluez la fonction de valeur de la politique aléatoire à l'aide d'un algorithme itératif. Arrếtez l'algorithme lorsque les valeurs n'ont pas évolué de plus de 1e-2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state values\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "Number of iterations:  17\n",
      "\n",
      "State values after policy evaluation\n",
      "[0, -3.2879383610186155, -4.44555114854303, -5.00589407010135, -4.994933400824104, -4.233009887479573]\n",
      "[-4.666149237691077, -5.752817907690666, -5.9498007272112545, -6.0743710383323, -6.109557938542456, -5.514759802492964]\n",
      "[-5.190389376976314, -6.274827946280725, -6.467204718420831, -6.585900914917751, -6.413431263750833, -5.7246285531996035]\n",
      "[-5.341258015743366, -6.444051839317244, -6.570493937301101, -6.622970332441267, -6.394948276774256, -5.708611497306631]\n",
      "[-5.287249963492066, -6.310811811652834, -6.313233609208383, -6.403270833598547, -6.181134103802791, -5.387044957210603]\n",
      "[-4.511604206204862, -5.817761496164923, -5.954749934720489, -5.367132997082065, -3.891576941655959, 0]\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import typing as t\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Action = t.Literal[\"L\", \"R\", \"U\" , \"D\"]\n",
    "CUBE_SIDE = 6\n",
    "\n",
    "@dataclass\n",
    "class State: \n",
    "    \"\"\"\n",
    "    It represents any cell in the world\n",
    "    \"\"\"\n",
    "    cell: int\n",
    "    value: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.bounds = {\n",
    "            'L': self.cell - self.cell % CUBE_SIDE,\n",
    "            'R': self.cell - self.cell % CUBE_SIDE + (CUBE_SIDE - 1),\n",
    "            'U': self.cell % CUBE_SIDE,\n",
    "            'D': self.cell % CUBE_SIDE + CUBE_SIDE * (CUBE_SIDE - 1),\n",
    "        }\n",
    "        self.neighbors = [self.act(a) for a in \"LRUD\"]\n",
    "        assert all(i >= 0 and i < CUBE_SIDE*CUBE_SIDE for i in self.neighbors)\n",
    "    \n",
    "    def is_termination(self):\n",
    "        return self.cell in {0, CUBE_SIDE * CUBE_SIDE - 1}\n",
    "\n",
    "    def act(self, a: Action):\n",
    "        \"\"\"\n",
    "        Get next state\n",
    "        \"\"\"\n",
    "        if a == 'L': \n",
    "            return min(self.bounds['R'], max(self.bounds['L'], self.cell - 1))\n",
    "        if a == 'R': \n",
    "            return min(self.bounds['R'], max(self.bounds['L'], self.cell + 1))\n",
    "        if a == 'U': \n",
    "            return min(self.bounds['D'], max(self.bounds['U'], self.cell - 4))\n",
    "        if a == 'D':\n",
    "            return min(self.bounds['D'], max(self.bounds['U'], self.cell + 4))\n",
    "        raise ValueError('Unexpected action')\n",
    "    \n",
    "\n",
    "def init_states():\n",
    "    return [State(i) for i in range(CUBE_SIDE * CUBE_SIDE)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Env:\n",
    "    states: t.List[State] = field(default_factory=init_states)\n",
    "\n",
    "def policy_evaluation(env, pi, gamma=0.9, theta=1e-2, r=-1):\n",
    "    \"\"\"\n",
    "    Policy evaluation function\n",
    "    \"\"\"\n",
    "    inter = 1\n",
    "    while True:\n",
    "        delta = 0\n",
    "        env_copy = Env(states=env.states.copy())\n",
    "\n",
    "        for s, s_copy in zip(env.states, env_copy.states):\n",
    "            if s.is_termination():\n",
    "                continue\n",
    "            v = s.value\n",
    "            s.value = 0\n",
    "\n",
    "            for i, index_state in enumerate(s_copy.neighbors):\n",
    "                s.value += pi[s.cell][i] * (r + gamma * env_copy.states[index_state].value)\n",
    "\n",
    "            delta = max(delta, abs(v - s.value))\n",
    "        if delta < theta:\n",
    "                break\n",
    "        inter += 1\n",
    "    print(\"Number of iterations: \", inter)\n",
    "    return env\n",
    "\n",
    "def print_states(states):\n",
    "    for i in range(CUBE_SIDE):\n",
    "        print([states[i*CUBE_SIDE + j].value for j in range(CUBE_SIDE)])\n",
    "env = Env()\n",
    "print(\"Initial state values\")\n",
    "print_states(env.states)\n",
    "\n",
    "# The probability of taking the action is 0.25 because we have 4 actions\n",
    "pol_eval = policy_evaluation(env, [[0.25, 0.25, 0.25, 0.25] for _ in range(CUBE_SIDE * CUBE_SIDE)])\n",
    "\n",
    "print(\"\\nState values after policy evaluation\")\n",
    "print_states(pol_eval.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La politique gloutonne cherche uniquement à exploiter, sans aucune exploration. A chaque instant, elle choisit l'action qui permet de maximiser la fonction de valeur :\n",
    "\n",
    "$$\\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
    "\n",
    "**Q2: calculez la politique ainsi obtenue. Vérifiez qu'il s'agit de la politique optimale. Combien d'itérations ont été nécessaires pour obtenir ce résultat ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:\n",
      "[[0, 0, 0, 0], [1.0, 0, 0, 0], [1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 1.0, 0, 0], [0, 0.5, 0.5, 0]]\n",
      "[[0, 0, 1.0, 0], [1.0, 0, 0, 0], [0, 0, 1.0, 0], [0, 0, 1.0, 0], [0, 0, 1.0, 0], [0, 1.0, 0, 0]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0, 0, 1.0, 0], [0, 0, 1.0, 0], [0, 1.0, 0, 0]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0, 0, 1.0, 0], [0, 0, 1.0, 0], [0, 1.0, 0, 0]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0, 0, 1.0, 0], [0, 0, 1.0, 0], [0, 0, 0, 1.0]]\n",
      "[[0.5, 0, 0, 0.5], [1.0, 0, 0, 0], [0, 1.0, 0, 0], [0, 1.0, 0, 0], [0, 1.0, 0, 0], [0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# compute policy with state values\n",
    "def compute_policy(env):\n",
    "    pi = [ [0, 0, 0, 0] for _ in range(CUBE_SIDE * CUBE_SIDE)]\n",
    "    for s in env.states:\n",
    "        if s.is_termination():\n",
    "            continue\n",
    "        for i, index_state in enumerate(s.neighbors):\n",
    "            pi[s.cell][i] = env.states[index_state].value\n",
    "        \n",
    "        max_value = max(pi[s.cell])\n",
    "\n",
    "        # Count number of max values to divide the probability\n",
    "        count = 0\n",
    "        for i in range(len(pi[s.cell])):\n",
    "            if pi[s.cell][i] == max_value:\n",
    "                count += 1\n",
    "\n",
    "        for i, value in enumerate(pi[s.cell]):\n",
    "            if value != max_value:\n",
    "                pi[s.cell][i] = 0\n",
    "            else:\n",
    "                pi[s.cell][i] = 1 / count\n",
    "    return pi\n",
    "\n",
    "policy = compute_policy(pol_eval)\n",
    "\n",
    "\n",
    "print(\"\\nPolicy:\")\n",
    "for i in range(CUBE_SIDE):\n",
    "    print([policy[i*CUBE_SIDE + j] for j in range(CUBE_SIDE)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on suit les probabilités trouvées on va bien se diriger de façon optimale vers la destination. Le nombre d'itérations effectué est 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Algorithme *policy iteration*\n",
    "\n",
    "Une amélioration de l'algorithme consiste 1) à évaluer la fonction de valeur sur un petit nombre d'itérations (on testera en Q3 avec une seule itération), puis 2) à mettre à jour la politique, puis à recommencer l'étape 1). On peut arrếter l'entraînement lorsque la politique a convergé.\n",
    "\n",
    "**Q3: implémentez cet algorithme. Est-il plus rapide ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state values\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "Number of iterations:  5 \n",
      "\n",
      "\n",
      "State values after policy evaluation\n",
      "[0, -1.0, -1.9, -2.71, -2.71, -3.439]\n",
      "[-1.0, -1.9, -1.9, -2.71, -1.9, -2.71]\n",
      "[-1.0, -1.9, -1.9, -2.71, -1.9, -2.71]\n",
      "[-1.0, -1.9, -1.9, -2.71, -1.9, -2.71]\n",
      "[-1.0, -1.9, -2.1025, -2.71, -1.9, -2.71]\n",
      "[-1.225, -2.1025, -2.71, -1.9, -1.0, 0]\n",
      "\n",
      "Policy:\n",
      "[[0, 0, 0, 0], [1.0, 0, 0, 0], [0.5, 0, 0, 0.5], [0.5, 0, 0, 0.5], [0, 0, 0, 1.0], [0.5, 0, 0, 0.5]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0.3333333333333333, 0.3333333333333333, 0, 0.3333333333333333], [0, 0, 1.0, 0], [0.5, 0, 0.5, 0]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0.3333333333333333, 0.3333333333333333, 0, 0.3333333333333333], [0, 0, 1.0, 0], [0.5, 0, 0.5, 0]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0.3333333333333333, 0.3333333333333333, 0, 0.3333333333333333], [0, 0, 1.0, 0], [0.5, 0, 0.5, 0]]\n",
      "[[1.0, 0, 0, 0], [1.0, 0, 0, 0], [0, 0, 0, 1.0], [0, 1.0, 0, 0], [0, 0, 1.0, 0], [0.3333333333333333, 0, 0.3333333333333333, 0.3333333333333333]]\n",
      "[[0.5, 0, 0, 0.5], [1.0, 0, 0, 0], [0, 0.5, 0.5, 0], [0, 1.0, 0, 0], [0, 1.0, 0, 0], [0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def improved_policy_evaluation(env, pi, iter_max, gamma=0.9, theta=1e-2, r=-1, iter=1):\n",
    "    \"\"\"\n",
    "    Improved policy evaluation function\n",
    "    \"\"\"\n",
    "    delta = 0\n",
    "    env_copy = Env(states=env.states.copy())\n",
    "\n",
    "    for s, s_copy in zip(env.states, env_copy.states):\n",
    "        if s.is_termination():\n",
    "            continue\n",
    "        v = s.value\n",
    "        s.value = 0\n",
    "        \n",
    "        for i, index_state in enumerate(s_copy.neighbors):\n",
    "            s.value += pi[s.cell][i] * (r + gamma * env_copy.states[index_state].value)\n",
    "        \n",
    "        delta = max(delta, abs(v - s.value))\n",
    "\n",
    "    if delta < theta or iter == iter_max:\n",
    "        print(\"Number of iterations: \", iter, \"\\n\")\n",
    "        return env\n",
    "    \n",
    "    pi_new = compute_policy(env)\n",
    "    return improved_policy_evaluation(env, pi_new, iter_max, gamma, theta, r, iter = iter + 1)\n",
    "\n",
    "env = Env()\n",
    "print(\"Initial state values\")\n",
    "print_states(env.states) \n",
    "\n",
    "pol_eval = improved_policy_evaluation(env, [[0.25, 0.25, 0.25, 0.25] for _ in range(CUBE_SIDE * CUBE_SIDE)], 100)\n",
    "\n",
    "print(\"\\nState values after policy evaluation\")\n",
    "print_states(pol_eval.states)\n",
    "\n",
    "policy = compute_policy(pol_eval)\n",
    "print(\"\\nPolicy:\")\n",
    "for i in range(CUBE_SIDE):\n",
    "    print([policy[i*CUBE_SIDE + j] for j in range(CUBE_SIDE)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut le voir grâce au nombre d'itération. Le premier algo est à 17 itérations alors qu'ici on est à 5 itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Algorithme *value iteration*\n",
    "\n",
    "Une autre variante conserve la politique aléatoire tout en long de l'entraînement, mais met à jour la fonction de valeur avec l'équation suivante :\n",
    "\n",
    "$$v_{k+1} (s) = \\max_{a} \\sum_{s'} p(s'|s, a)\\left[r+\\gamma v_k(s') \\right]$$\n",
    "\n",
    "Une fois que la fonction de valeur a convergé, on calcule la politique avec :\n",
    "\n",
    "$$\\pi(s) = argmax_a \\sum_{s'} p(s'|s,a)[r+\\gamma V(s')]$$\n",
    "\n",
    "\n",
    "**Q4: implémentez cet algorithme.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state values\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "Number of iterations:  4 \n",
      "\n",
      "\n",
      "State values after policy evaluation\n",
      "[0, -0.25, -0.30625, -0.31890625, -0.32175390625, -0.32175390625]\n",
      "[-0.31890625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625]\n",
      "[-0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625]\n",
      "[-0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625]\n",
      "[-0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.32175390625, -0.31890625]\n",
      "[-0.32175390625, -0.32175390625, -0.31890625, -0.30625, -0.25, 0]\n"
     ]
    }
   ],
   "source": [
    "# computes the greedy policy evaluation\n",
    "def greedy_policy_evaluation(env, pi, gamma=0.9, theta=1e-2, r=-1, iter=1):\n",
    "    \"\"\"\"\n",
    "    Greddy policy evaluation function\n",
    "    \"\"\"\n",
    "    delta = 0\n",
    "    env_copy = Env(states=env.states.copy())\n",
    "\n",
    "    for s, s_copy in zip(env.states, env_copy.states):\n",
    "        if s.is_termination():\n",
    "            continue\n",
    "        v = s.value\n",
    "        s.value = -float('inf')\n",
    "        \n",
    "        for i, index_state in enumerate(s_copy.neighbors):\n",
    "            s.value = max(s.value, pi[s.cell][i] * (r + gamma * env_copy.states[index_state].value))\n",
    "        \n",
    "        delta = max(delta, abs(v - s.value))\n",
    "\n",
    "    if delta < theta:\n",
    "        print(\"Number of iterations: \", iter, \"\\n\")\n",
    "        return env\n",
    "\n",
    "    return greedy_policy_evaluation(env, pi, gamma, theta, r, iter = iter + 1)\n",
    "\n",
    "env = Env()\n",
    "print(\"Initial state values\")\n",
    "print_states(env.states) \n",
    "\n",
    "pol_eval = greedy_policy_evaluation(env, [[0.25, 0.25, 0.25, 0.25] for _ in range(CUBE_SIDE * CUBE_SIDE)])\n",
    "\n",
    "print(\"\\nState values after policy evaluation\")\n",
    "print_states(pol_eval.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir avec le nombre d'itération que cette dernière implémentation semble être la plus rapide. Cependant, la politique reste aléatoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: Quel algorithme vous paraît le plus judicieux ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme qui parait être le meilleur car il est optimisé et les résultats sont cohérents. En effet on est sûr de s'arrêter au bon moment et d'avoir des résultats plus qu'acceptable dans la prise de décision."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1612589585.625011,
  "download_nb": false,
  "filename": "20_markov.rst",
  "filename_with_path": "20_markov",
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "title": "Foundations of Computational Economics #20",
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
