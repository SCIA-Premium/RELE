{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55339855-cf14-4697-a122-4b54a92c6f6b",
   "metadata": {},
   "source": [
    "# TP4 : model-free RL \n",
    "\n",
    "Jusqu'à lors, nous avons étudié des cas où le modèle de la matrice de transition et de la récompense étaient modélisés : c'est le cadre du *model-based reinforcement learning*. \n",
    "Pour gérer des cas plus complexes, nous devons soulager ces hypothèses. Dans le *model-free reinforcement learning*, on regarde les expériences passées, échecs comme réussites, pour simuler le modèle sous-jacent.\n",
    "\n",
    "Ce TP propose d'implémenter plusieurs algorithmes allant dans ce sens.\n",
    "\n",
    "\n",
    "RAPPEL : 1/4 de la note finale est liée à la mise en forme : \n",
    "\n",
    "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
    "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
    "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
    "\n",
    "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
    "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
    "\n",
    "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
    "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
    "\n",
    "```\n",
    "Nom1, Prenom1, Email1, NumEtudiant1\n",
    "Nom2, Prenom2, Email2, NumEtudiant2\n",
    "Nom3, Prenom3, Email3, NumEtudiant3\n",
    "Nom4, Prenom4, Email4, NumEtudiant4\n",
    "```\n",
    "\n",
    "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f9115-21f6-4931-8920-e3be1555e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy matplotlib 'gym[toy_text, classic_control]'\n",
    "# Pour gérer les dépendances de PyGame: https://www.pygame.org/wiki/Compilation\n",
    "# A cause de PyGame, la version de Python doit être inférieure à 3.10 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ede7081-b933-476b-9d38-87d752a87777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ea33d-04c0-401e-af52-f972be093985",
   "metadata": {},
   "source": [
    "## Présentation de Gym\n",
    "Dans ce TP, nous allons utiliser le simulateur Gym par Open AI. Gym fournit une série d'environnements qui ont permis aux chercheurs de se comparer et ainsi, de faire accélérer la recherche en reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bbfe25c-6c12-46ac-9c54-6229d5ba8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # Take a random action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b0202-7a52-4648-87fa-dfccd1d6e179",
   "metadata": {},
   "source": [
    "**Q1. Décrire l'environnement Taxi. En particulier, vous vous demanderez quel est l'espace d'action ? Des états ? Comment est définie la récompense ? Que contient l'output de `env.step` ?**\n",
    "\n",
    "La documentation de cet environnement est disponible ici:\n",
    "    [https://www.gymlibrary.dev/api/core/#gym.Env.step](https://www.gymlibrary.dev/api/core/#gym.Env.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208b78d-bbcb-4023-95db-4622e47bca86",
   "metadata": {},
   "source": [
    "L'environnement Taxi est un parking avec plusieurs éléments. Il y a quatre emplacements désignés dans le monde de la grille indiqués par R (ed), G (reen), Y (ellow) et B (lue). Lorsque l'épisode commence, le taxi démarre sur une case aléatoire et le passager se trouve à un endroit aléatoire. Le taxi se rend à l'emplacement du passager, prend le passager, le conduit jusqu'à sa destination (un autre des quatre emplacements spécifiés), puis dépose le passager. Une fois le passager déposé, l'épisode se termine.\n",
    "\n",
    "Il existe 6 actions déterministes discrètes :\n",
    "* 0 : se déplacer vers le sud\n",
    "* 1 : se déplacer vers le nord\n",
    "* 2 : se déplacer vers l'est\n",
    "* 3 : se déplacer vers l'ouest\n",
    "* 4 : passager de ramassage\n",
    "* 5 : déposer un passager\n",
    "\n",
    "L'output de `env.step` est un Tuple[ObsType, float, bool, bool, dict], il contient :\n",
    "\n",
    "* observation (objet) - ce sera un élément de l'espace d'observation de l'environnement. Cela peut, par exemple, être un tableau numpy contenant les positions et les vitesses de certains objets.\n",
    "* reward (float) - Le montant de la récompense retournée à la suite de l'action.\n",
    "* terminated (bool) – si un état terminal est atteint. Dans ce cas, d'autres appels step() pourraient renvoyer des résultats indéfinis.\n",
    "* truncated (bool) – Généralement une limite de temps, mais peut également être utilisée pour indiquer que l'agent sort physiquement des limites. Peut être utilisé pour mettre fin à l'épisode prématurément avant qu'un état terminal ne soit atteint.\n",
    "* info (dict) - info contient des informations de diagnostic auxiliaires (utiles pour le débogage, l'apprentissage et la journalisation). Cela peut, par exemple, contenir : des métriques qui décrivent l'état de performance de l'agent, des variables qui sont masquées dans les observations ou des termes de récompense individuels qui sont combinés pour produire la récompense totale. Il peut également contenir des informations qui distinguent la troncature et la terminaison, mais cela est déconseillé en faveur du retour de deux booléens, et sera supprimé dans une future version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90446e1c-0cd7-423e-b887-591cd39ee473",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172a4c1-1fac-4cc0-ad4f-eaf34ff6c070",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons utiliser le Q-Learning pour résoudre cette tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb761a5-c886-45d4-96c6-1c8b86037773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward -9.4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "def play_and_train(env: gym.core.Env, agent: QLearningAgent, t_max=int(1e4)) -> float:\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total rewardb\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s, info = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # TODO get agent to pick action given state s\n",
    "        a = agent.get_action(s)\n",
    "        \n",
    "        next_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        # TODO train agent for state s\n",
    "        agent.update(s, a, r, next_s)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))    \n",
    "    if i % 100 ==0:\n",
    "        clear_output(True)\n",
    "        print(\"mean reward\",np.mean(rewards[-100:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b6f0e-2615-475c-ad5b-6617222e5842",
   "metadata": {},
   "source": [
    "**Q2. Complétez les bouts manquants de code dans `qlearning.py` et dans la fonction `play_and_train`. Entrainez l'agent. Que se passe-t'il ? Regardez des exemples de trajectoire.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f217a9-0824-48dd-933d-5adf7e27b1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48490cf6-0dc5-4f03-a354-7e54058ca0a4",
   "metadata": {},
   "source": [
    "## Réduire epsilon au-fur-et-à-mesure\n",
    "\n",
    "Pour améliorer les performances, nous allons réduire $\\epsilon$ au cours du temps.\n",
    "\n",
    "La manière la plus simple consiste à réduire $\\epsilon$ à chaque épisode, par exemple en le multipliant par un nombre proche de 1 (tel que 0.99) ou lui soustraire un pettit nombre. Vous pouvez, bien sûr, envisager d'autres stratégies !\n",
    "\n",
    "**Q3. Améliorez l'algorithme dans `q_learning_eps_scheduling.py` de sorte à avoir une récompense positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd6baf-a206-47bd-b978-f3e9cd59fde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q_learning_eps_scheduling import QLearningAgentEpsScheduling\n",
    "\n",
    "agent = QLearningAgentEpsScheduling(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))    \n",
    "    if i %100 ==0:\n",
    "        clear_output(True)\n",
    "        print(\"mean reward\",np.mean(rewards[-100:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe545baf-9a53-47cc-8d47-18a886b933f4",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7106d2e-55c6-4ffa-a732-a32e12159dce",
   "metadata": {},
   "source": [
    "**Q4. Produisez quelques vidéos des trajectoires obtenus. Rassemblez des cas de réussite et des cas d'échecs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbe8e3-1790-441d-8b43-57f18df41e4d",
   "metadata": {},
   "source": [
    "## Espace d'action continu\n",
    "\n",
    "Nous allons maintenant passer à un environnement plus difficile : le pendule inversé. C'est un grand classique des problèmes de contrôle !\n",
    "\n",
    "Puisque l'environnement a un espace d'actions continu, nous allons discrétiser cet espace pour revenir aux cas précédemment étudiés.\n",
    "La solution la plus simple pour cela consiste à diviser l'espace en sections égales. Mais comment choisir le nombre de sections ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b6ee-297a-4136-ab66-59adb3f2dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"first state: \", env.reset())\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5417ec-eb7e-48fb-aa0c-6bbc8fd5b436",
   "metadata": {},
   "source": [
    "**Q5. Décrivez cet environnement à l'aide de la documentation de Gym en reprenant la Q1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67baed-4bfc-4a59-a76a-dbbc19cbbd2c",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514cb53-9981-4fd5-8f7d-652de722c0a3",
   "metadata": {},
   "source": [
    "Pour mener à bien notre discrétisation de l'espace des actions, nous allons estimer la distribution des observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdee467-d593-4be0-85b5-20c11b2725d0",
   "metadata": {},
   "source": [
    "**Q6. Simulez 1000 épisodes et regardez la distribution des états à l'aide d'un histogramme. Quel paramètre vous paraît optimal pour choisir le nombre de `bins` ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac1824-18e8-419f-9e47-cea3926512bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a231538-b6a3-4420-9b73-965d958c8d74",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff90a63-5b02-4055-bdca-7a6a9c8eb1aa",
   "metadata": {},
   "source": [
    "## Discrétiser l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef258ed-07aa-452a-94f8-dcce941915a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "    def _observation(self, state: np.ndarray) -> np.ndarray:  \n",
    "        # TODO binarize each dimension of the state\n",
    "        return state\n",
    "    \n",
    "bi_env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e08b3-ca10-4377-9d86-2571439f5cbb",
   "metadata": {},
   "source": [
    "**Q7. Complétez le code ci-dessus pour binariser l'environnement. Regardez la nouvelle distribution des états. Qu'en pensez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4be48-f1d8-4eab-a271-957c8b45d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "598a8ca4-d6b4-407c-8cfc-c870732fbb4a",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1b63f-465a-4006-96c3-35d8e921c25e",
   "metadata": {},
   "source": [
    "### Apprentissage sur l'environnement discretisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9944bf8-830c-4645-9df6-3fd471360b2e",
   "metadata": {},
   "source": [
    "**Q8. Reprenez votre agent pour résoudre cette tâche. Tracez la récompense et observez quelques trajectoires d'échecs et de réussites à l'aide de vidéos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da18223-f328-48f3-9643-e1c9dba17ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0627c3b-2f9b-434b-9f14-3e30615749e5",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af0361-953c-4a03-8e7c-49840c2c9426",
   "metadata": {},
   "source": [
    "## 4. Experience replay \n",
    "\n",
    "Les algorithmes *off-policy* peuvent s'entraîner grâce à des trajectoires anciennement obtenues. Tirons parti de cette propriété pour \n",
    "améliorer l'efficacité de nos algorithmes et les permettre de converger plus rapidement.\n",
    "\n",
    "L'idée générale est de collecter les tuplets `<s,a,r,s'>` dans un *buffer*, puis de mettre à jour la fonction Q sur l'ensemble de ces tuplets.\n",
    "Plus en détails, voici l'algorithme à suivre\n",
    "\n",
    "#### S'entraîner avec un *experience replay*\n",
    "1. Echantillonner un tuplet `<s,a,r,s'>`.\n",
    "2. Stocker ce tuplet dans un buffer FIFO : si le buffer est plein, on supprimer les données arrivées en premier.\n",
    "3. Choisir aléatoirement K tuplets du buffer et mettre à jour la fonction Q sur ces tuplets.\n",
    "\n",
    "**Q9. Implémentez un tel buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef979349-b4eb-4d4a-9a5b-4575a0e04d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "            \n",
    "        Note: for this assignment you can pick any data structure you want.\n",
    "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
    "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "# Some tests to make sure your buffer works right\n",
    "\n",
    "replay = ReplayBuffer(2)\n",
    "obj1 = tuple(range(5))\n",
    "obj2 = tuple(range(5, 10))\n",
    "replay.add(*obj1)\n",
    "assert replay.sample(1) == obj1, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage) == 2, \"Please make sure __len__ methods works as intended.\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
    "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2\n",
    "replay.add(*obj1)\n",
    "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n",
    "replay.add(*obj1)\n",
    "assert tuple(np.unique(a) for a in replay.sample(100)) == obj1\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce711801-4660-44f1-8189-cd91187bf5f0",
   "metadata": {},
   "source": [
    "Maintenant utilisons ce replay buffer pour améliorer les performances d'entraînement.\n",
    "\n",
    "**Q10. Entraînez un agent avec le replay buffer. Comparez l'évolution de la récompense sur l'environnement Taxi avec un algorithme n'utilisant pas le replay buffer.**\n",
    "\n",
    "Pour rendre l'affichage plus visible, vous filterez la récompense avec un filtre de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34cece-905d-4597-ae71-903c68f11296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_baseline = QLearningAgentEpsScheduling(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "agent_replay = QLearningAgentEpsScheduling(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "replay = ReplayBuffer(10000)\n",
    "\n",
    "def play_and_train(\n",
    "    env: gym.core.Env, \n",
    "    agent: QLearningAgent, \n",
    "    t_max: int = int(1e4),\n",
    "    batch_size: int = 32\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total rewardb\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s, info = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # TODO get agent to pick action given state s\n",
    "        a = 0\n",
    "        \n",
    "        next_s, r, done, truncated, info = env.step(a)\n",
    "        \n",
    "        # TODO train agent for state s\n",
    "        # ...\n",
    "        if replay is not None:\n",
    "            # TODO update this part\n",
    "            pass\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da335a-8376-44c1-8ac4-da691cad9b6b",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('usr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
