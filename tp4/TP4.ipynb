{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55339855-cf14-4697-a122-4b54a92c6f6b",
   "metadata": {},
   "source": [
    "# TP4 : model-free RL \n",
    "\n",
    "Jusqu'à lors, nous avons étudié des cas où le modèle de la matrice de transition et de la récompense étaient modélisés : c'est le cadre du *model-based reinforcement learning*. \n",
    "Pour gérer des cas plus complexes, nous devons soulager ces hypothèses. Dans le *model-free reinforcement learning*, on regarde les expériences passées, échecs comme réussites, pour simuler le modèle sous-jacent.\n",
    "\n",
    "Ce TP propose d'implémenter plusieurs algorithmes allant dans ce sens.\n",
    "\n",
    "\n",
    "RAPPEL : 1/4 de la note finale est liée à la mise en forme : \n",
    "\n",
    "* pensez à nettoyer les outputs inutiles (installation, messages de débuggage, ...)\n",
    "* soignez vos figures : les axes sont-ils faciles à comprendre ? L'échelle est adaptée ? \n",
    "* commentez vos résultats : vous attendiez-vous à les avoir ? Est-ce étonnant ? Faites le lien avec la théorie.\n",
    "\n",
    "Ce TP reprend l'exemple d'un médecin et de ses vaccins. Vous allez comparer plusieurs stratégies et trouver celle optimale.\n",
    "Un TP se fait en groupe de 2 à 4. Aucun groupe de plus de 4 personnes. \n",
    "\n",
    "Vous allez rendre le TP dans une archive ZIP. L'archive ZIP contient ce notebook au format `ipynb`, mais aussi exporté en PDF & HTML. \n",
    "L'archive ZIP doit aussi contenir un fichier txt appelé `groupe.txt` sous le format:\n",
    "\n",
    "```\n",
    "Nom1, Prenom1, Email1, NumEtudiant1\n",
    "Nom2, Prenom2, Email2, NumEtudiant2\n",
    "Nom3, Prenom3, Email3, NumEtudiant3\n",
    "Nom4, Prenom4, Email4, NumEtudiant4\n",
    "```\n",
    "\n",
    "Un script vient extraire vos réponses : ne changez pas l'ordre des cellules et soyez sûrs que les graphes sont bien présents dans la version notebook soumise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f9115-21f6-4931-8920-e3be1555e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy matplotlib 'gym[toy_text, classic_control]'\n",
    "# Pour gérer les dépendances de PyGame: https://www.pygame.org/wiki/Compilation\n",
    "# A cause de PyGame, la version de Python doit être inférieure à 3.10 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede7081-b933-476b-9d38-87d752a87777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ea33d-04c0-401e-af52-f972be093985",
   "metadata": {},
   "source": [
    "## Présentation de Gym\n",
    "Dans ce TP, nous allons le simulateur Gym par Open AI. Gym fournit une série d'environnements qui ont permis aux chercheurs de se comparer et ainsi, de faire accélérer la recherche en reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfe25c-6c12-46ac-9c54-6229d5ba8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset()\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # Take a random action\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b0202-7a52-4648-87fa-dfccd1d6e179",
   "metadata": {},
   "source": [
    "**Q1. Décrire l'environnement Taxi. En particulier, vous vous demanderez quel est l'espace d'action ? Des états ? Comment est définie la récompense ? Que contient l'output de `env.step` ?**\n",
    "\n",
    "La documentation de cet environnement est disponible ici:\n",
    "    [https://www.gymlibrary.dev/api/core/#gym.Env.step](https://www.gymlibrary.dev/api/core/#gym.Env.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208b78d-bbcb-4023-95db-4622e47bca86",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90446e1c-0cd7-423e-b887-591cd39ee473",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172a4c1-1fac-4cc0-ad4f-eaf34ff6c070",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons utiliser le Q-Learning pour résoudre cette tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb761a5-c886-45d4-96c6-1c8b86037773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "def play_and_train(env: gym.core.Env, agent: QLearningAgent, t_max=int(1e4)) -> float:\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total rewardb\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s, info = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # TODO get agent to pick action given state s\n",
    "        a = 0\n",
    "        \n",
    "        next_s, r, done, truncated, info = env.step(a)\n",
    "        \n",
    "        # TODO train agent for state s\n",
    "        # ...\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))    \n",
    "    if i % 100 ==0:\n",
    "        clear_output(True)\n",
    "        print(\"mean reward\",np.mean(rewards[-100:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b6f0e-2615-475c-ad5b-6617222e5842",
   "metadata": {},
   "source": [
    "**Q2. Complétez les bouts manquants de code dans `qlearning.py` et dans la fonction `play_and_train`. Entrainez l'agent. Que se passe-t'il ? Regardez des exemples de trajectoire.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f217a9-0824-48dd-933d-5adf7e27b1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48490cf6-0dc5-4f03-a354-7e54058ca0a4",
   "metadata": {},
   "source": [
    "## Réduire epsilon au-fur-et-à-mesure\n",
    "\n",
    "Pour améliorer les performances, nous allons réduire $\\epsilon$ au cours du temps.\n",
    "\n",
    "La manière la plus simple consiste à réduire $\\epsilon$ à chaque épisode, par exemple en le multipliant par un nombre proche de 1 (tel que 0.99) ou lui soustraire un pettit nombre. Vous pouvez, bien sûr, envisager d'autres stratégies !\n",
    "\n",
    "**Q3. Améliorez l'algorithme dans `q_learning_eps_scheduling.py` de sorte à avoir une récompense positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd6baf-a206-47bd-b978-f3e9cd59fde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from q_learning_eps_scheduling import QLearningAgentEpsScheduling\n",
    "\n",
    "agent = QLearningAgentEpsScheduling(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env,agent))    \n",
    "    if i %100 ==0:\n",
    "        clear_output(True)\n",
    "        print(\"mean reward\",np.mean(rewards[-100:]))\n",
    "        plt.plot(rewards)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe545baf-9a53-47cc-8d47-18a886b933f4",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7106d2e-55c6-4ffa-a732-a32e12159dce",
   "metadata": {},
   "source": [
    "**Q4. Produisez quelques vidéos des trajectoires obtenus. Rassemblez des cas de réussite et des cas d'échecs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbe8e3-1790-441d-8b43-57f18df41e4d",
   "metadata": {},
   "source": [
    "## Espace d'action continu\n",
    "\n",
    "Nous allons maintenant passer à un environnement plus difficile : le pendule inversé. C'est un grand classique des problèmes de contrôle !\n",
    "\n",
    "Puisque l'environnement a un espace d'actions continu, nous allons discrétiser cet espace pour revenir aux cas précédemment étudiés.\n",
    "La solution la plus simple pour cela consiste à diviser l'espace en sections égales. Mais comment choisir le nombre de sections ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b6ee-297a-4136-ab66-59adb3f2dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"first state: \", env.reset())\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5417ec-eb7e-48fb-aa0c-6bbc8fd5b436",
   "metadata": {},
   "source": [
    "**Q5. Décrivez cet environnement à l'aide de la documentation de Gym en reprenant la Q1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67baed-4bfc-4a59-a76a-dbbc19cbbd2c",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514cb53-9981-4fd5-8f7d-652de722c0a3",
   "metadata": {},
   "source": [
    "Pour mener à bien notre discrétisation de l'espace des actions, nous allons estimer la distribution des observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdee467-d593-4be0-85b5-20c11b2725d0",
   "metadata": {},
   "source": [
    "**Q6. Simulez 1000 épisodes et regardez la distribution des états à l'aide d'un histogramme. Quel paramètre vous paraît optimal pour choisir le nombre de `bins` ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac1824-18e8-419f-9e47-cea3926512bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a231538-b6a3-4420-9b73-965d958c8d74",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff90a63-5b02-4055-bdca-7a6a9c8eb1aa",
   "metadata": {},
   "source": [
    "## Discrétiser l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef258ed-07aa-452a-94f8-dcce941915a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "    def _observation(self, state: np.ndarray) -> np.ndarray:  \n",
    "        # TODO binarize each dimension of the state\n",
    "        return state\n",
    "    \n",
    "bi_env = Binarizer(gym.make(\"CartPole-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e08b3-ca10-4377-9d86-2571439f5cbb",
   "metadata": {},
   "source": [
    "**Q7. Complétez le code ci-dessus pour binariser l'environnement. Regardez la nouvelle distribution des états. Qu'en pensez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4be48-f1d8-4eab-a271-957c8b45d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "598a8ca4-d6b4-407c-8cfc-c870732fbb4a",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1b63f-465a-4006-96c3-35d8e921c25e",
   "metadata": {},
   "source": [
    "### Apprentissage sur l'environnement discretisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9944bf8-830c-4645-9df6-3fd471360b2e",
   "metadata": {},
   "source": [
    "**Q8. Reprenez votre agent pour résoudre cette tâche. Tracez la récompense et observez quelques trajectoires d'échecs et de réussites à l'aide de vidéos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da18223-f328-48f3-9643-e1c9dba17ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0627c3b-2f9b-434b-9f14-3e30615749e5",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af0361-953c-4a03-8e7c-49840c2c9426",
   "metadata": {},
   "source": [
    "## 4. Experience replay \n",
    "\n",
    "Les algorithmes *off-policy* peuvent s'entraîner grâce à des trajectoires anciennement obtenues. Tirons parti de cette propriété pour \n",
    "améliorer l'efficacité de nos algorithmes et les permettre de converger plus rapidement.\n",
    "\n",
    "L'idée générale est de collecter les tuplets `<s,a,r,s'>` dans un *buffer*, puis de mettre à jour la fonction Q sur l'ensemble de ces tuplets.\n",
    "Plus en détails, voici l'algorithme à suivre\n",
    "\n",
    "#### S'entraîner avec un *experience replay*\n",
    "1. Echantillonner un tuplet `<s,a,r,s'>`.\n",
    "2. Stocker ce tuplet dans un buffer FIFO : si le buffer est plein, on supprimer les données arrivées en premier.\n",
    "3. Choisir aléatoirement K tuplets du buffer et mettre à jour la fonction Q sur ces tuplets.\n",
    "\n",
    "**Q9. Implémentez un tel buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef979349-b4eb-4d4a-9a5b-4575a0e04d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "            \n",
    "        Note: for this assignment you can pick any data structure you want.\n",
    "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
    "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "# Some tests to make sure your buffer works right\n",
    "\n",
    "replay = ReplayBuffer(2)\n",
    "obj1 = tuple(range(5))\n",
    "obj2 = tuple(range(5, 10))\n",
    "replay.add(*obj1)\n",
    "assert replay.sample(1) == obj1, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage) == 2, \"Please make sure __len__ methods works as intended.\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
    "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2\n",
    "replay.add(*obj1)\n",
    "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n",
    "replay.add(*obj1)\n",
    "assert tuple(np.unique(a) for a in replay.sample(100)) == obj1\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce711801-4660-44f1-8189-cd91187bf5f0",
   "metadata": {},
   "source": [
    "Maintenant utilisons ce replay buffer pour améliorer les performances d'entraînement.\n",
    "\n",
    "**Q10. Entraînez un agent avec le replay buffer. Comparez l'évolution de la récompense sur l'environnement Taxi avec un algorithme n'utilisant pas le replay buffer.**\n",
    "\n",
    "Pour rendre l'affichage plus visible, vous filterez la récompense avec un filtre de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34cece-905d-4597-ae71-903c68f11296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_baseline = QLearningAgentEpsScheduling(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "agent_replay = QLearningAgentEpsScheduling(\n",
    "    learning_rate=0.5,\n",
    "    epsilon=0.25,\n",
    "    gamma=0.99,\n",
    "    legal_actions=list(range(n_actions))\n",
    ")\n",
    "\n",
    "replay = ReplayBuffer(10000)\n",
    "\n",
    "def play_and_train(\n",
    "    env: gym.core.Env, \n",
    "    agent: QLearningAgent, \n",
    "    t_max: int = int(1e4),\n",
    "    batch_size: int = 32\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total rewardb\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s, info = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # TODO get agent to pick action given state s\n",
    "        a = 0\n",
    "        \n",
    "        next_s, r, done, truncated, info = env.step(a)\n",
    "        \n",
    "        # TODO train agent for state s\n",
    "        # ...\n",
    "        if replay is not None:\n",
    "            # TODO update this part\n",
    "            pass\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da335a-8376-44c1-8ac4-da691cad9b6b",
   "metadata": {},
   "source": [
    "*[Ajoutez votre réponse ici]*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
